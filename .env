# Server Configuration
PORT=3000
NODE_ENV=development

# Azure Configuration
AZURE_TENANT_ID=your-tenant-id
AZURE_CLIENT_ID=your-client-id
AZURE_CLIENT_SECRET=your-client-secret
AZURE_SUBSCRIPTION_ID=your-subscription-id
AZURE_RESOURCE_GROUP=your-resource-group
AZURE_VMSS_NAME=your-vmss-name
AZURE_VM_SIZE=Standard_D2s_v3 # Optional: Used for memory % calculation in metrics

# Azure Storage Configuration (Optional: if metrics are stored in blob)
AZURE_STORAGE_ACCOUNT=
AZURE_STORAGE_ACCESS_KEY=
AZURE_MODEL_CONTAINER=models
AZURE_METRICS_CONTAINER=metrics

# Data Management
DATA_DIR=./data
DAYS_TO_FETCH=7

# Model Configuration
MODEL_PATH=file://./models
PREDICTION_HORIZON_HOURS=24
LOOKBACK_WINDOW=24
PREDICTION_THRESHOLD=0.7
MIN_INSTANCES=2
MAX_INSTANCES=10
MODEL_VERSION_TAG=
TRAINING_EPOCHS=50

# Metrics Collection
METRICS_PATH=./data
METRICS_INTERVAL_MIN=15
METRICS_LOOKBACK_HOURS=1

# Ollama Configuration
OLLAMA_API_URL=http://localhost:11434
OLLAMA_MODEL=llama3:8b # Using llama3:8b for infrastructure scaling
OLLAMA_FALLBACK_MODEL=mistral:7b # Fallback to mistral:7b if llama3:8b fails
OLLAMA_REQUEST_TIMEOUT=90000 # Reduced timeout for faster responses with 8B model (90 seconds)
OLLAMA_RETRY_COUNT=3 # Number of retry attempts for API calls
OLLAMA_RETRY_DELAY=1000 # Base delay between retries in milliseconds (increases with backoff)
OLLAMA_SYSTEM_PROMPT="You are CloudScaleGPT, an expert Azure cloud engineer specializing in infrastructure scaling and optimization. Analyze the provided metrics focusing on CPU usage trends, memory usage patterns, and network I/O to recommend an optimal VM instance count. Consider both performance needs and cost efficiency. Respond ONLY with a JSON object containing 'recommended_instances' (integer), 'confidence' (float between 0-1), and 'reasoning' (brief explanation)."

# Alternative Models (install via 'ollama pull MODEL_NAME')
# OLLAMA_MODEL=llama3:70b   # Higher accuracy but more resource intensive
# OLLAMA_MODEL=mixtral:8x7b # Strong reasoning for complex metrics analysis
# OLLAMA_MODEL=gemma:7b     # Efficient model with good reasoning

# Scaling Configuration
SCALING_CHECK_INTERVAL=300 # Seconds between scaling checks
SCALING_RETRY_COUNT=3      # Number of retry attempts for scaling operations
SCALING_COOLDOWN_MINUTES=15 # Minutes to wait between scaling operations
SCALING_CONFIDENCE_THRESHOLD=0.65 # Slightly lowered threshold for llama3:8b (0.7 â†’ 0.65)

# Logging Configuration
LOG_LEVEL=info
LOG_FILE=app.log

# Application Settings (Legacy/Unused)
# CACHE_TTL=600000 
# ENABLE_INSIGHTS=true 